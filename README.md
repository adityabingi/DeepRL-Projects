# Udacity-DRLND

My tensorflow 2.0 implementations  of RL and DeepRL projects and solutions to the exercises from open-source repo of Udacity DeepRL Nanodegree.I worked
myself through the Udacity DeepRL projects and I suggest anyone with knowledge of RL and Deep Learning to experiment with these open-source probelm exercises 
and projects.Although Udacity repo is based on pytorch I implement all the projects and exercise solutions in Tensorflow 2.0

# Table of Contents

## Exercise Solution Notebooks
[Discretization](discretization/Discretization.ipynb) - Continuous state and action spaces are discretized and Rl algorithms designed for discrete spaces are applied. This notebook solves for gym MountainCar-v0 env.

[Cross-Entropy-Method](cross-entropy/CEM.ipynb)  - Evolutionary optimization method CEM is applied for MountainCarContinuous Env

[Hill-Climbing](hill-Climbing/Hill_Climbing.ipynb) - Adaptive noise scaling with hill-climbing optimization applied to solve CartPole Env

[Tile-Coding](tile-coding/Tile_Coding.ipynb) - Tile coding is used to discretize continuous space to solve Acrobot-v1 Env

[Monte-Carlo](monte-carlo/Monte_Carlo.ipynb) - Monte-Carlo prediction & Monte-Carlo control used to solve BlackJack Env

[Temporal-Difference Methods](temporal-difference/Temporal_Difference.ipynb) - TD Methods SARSA, Q-Learning and Expected SARSA methods applied to solve CilffWalking Env

[Mini-Project-Taxiv3-Env](mini-project-taxiv3/) - TD Method Q-Learning used to solve Taxi-v3 Env

[DQN](dqn/) - Deep Q Learning (Q Learning with deep neural networks) applied to solve LunarLander Env

[Reinforce-policygradient](reinforce/) - Reinforce or Vanilla Policy Gradient method applied to solve CartPole Env

## Projects 

[Project1-Naviagation](project1_navigation/)

[Project2-Continuous Control](project2_continuous-control/)   - Will be updated soon

[Project3 - Collaboration and Competition](project3_colab-compete/)  - Will be updated soon


# Results
